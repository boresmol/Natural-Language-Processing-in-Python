{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Processing in Python\n",
        "## Autor: Borja Esteve\n",
        "Este módulo del curso se centra en aprender los conceptos básicos de NLP tales como:\n",
        "\n",
        "\n",
        "*   Identificación de temas\n",
        "*   Clasificación de textos\n",
        "*   ...\n",
        "\n",
        "\n",
        "## **1. Regular Expressions**\n",
        "Las expresiones regulares son cadenas que puede usar y que tienen una sintaxis especial, que te permite hacer coincidir patrones y encontrar otras cadenas.\n",
        "\n",
        "Un patrón es una serie de letras o símbolos que pueden corresponder a un texto, palabras o puntuación reales.\n",
        "\n",
        "Se pueden usar expresiones regulares para hacer cosas como buscar enlaces en una web, analizar direcciones de correo electrónico o eliminar cadenas o carácteres no deseados.  \n",
        "\n",
        "Estas expresioens regulares pueden usarse fácilmente con la biblioteca de Python `re`\n",
        "\n",
        "Podemos hacer coincidir una subcadena con el método de coincidencia que hace coincidir un patrón con una cadena:"
      ],
      "metadata": {
        "id": "objO2LnXtNa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEBjAadVk0qP",
        "outputId": "f3684dda-c83b-4129-858d-5159921975c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re\n",
        "re.match('abc', 'abcdef')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este método toma como primer aqrgumento el patrón, la cadena como el segundo y devuelve un objeto de coincidencia. Aquí vemos que coincidió exactamente con lo que esperábamos: abc.\n",
        "\n",
        "También podemos usar patrones especiales que regex enteinda, como `'\\w+'` el cual coincidirá con una palabra."
      ],
      "metadata": {
        "id": "msE_K2Hau4tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_regex = '\\w+'\n",
        "re.match(word_regex, 'hi there!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAg2gh0-vS6H",
        "outputId": "8ed34aa9-74fd-45ec-cf88-df704e76ec78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='hi'>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver aquí a través de la representación del objeto de coincidencia que ha coincidido con la primera palabra que encontró: hi\n",
        "\n",
        "| pattern | matches | examples |\n",
        "|-----------|-----------|-----------|\n",
        "| \\w+ | word | 'Magic' |\n",
        "| \\d | digit | 9 |\n",
        "| \\s | space | ' ' |\n",
        "| .* | wildcard | 'username74' |\n",
        "| + or * | greedy match | 'aaaaaa' |\n",
        "| \\S | not space | 'no_spaces' |\n",
        "| [a-z] | lowercase group | 'abcdefg |\n",
        "\n",
        "Algunas funciones de la librería que utilizaremos en el módulo serán:\n",
        "\n",
        "* `split`: parte un string en un regex\n",
        "* `findall`: busca todos los patrones en un string\n",
        "* `search`: busca un patrón\n",
        "* `match`: *matchea* un string entero o un substring en base a un patrón\n",
        "\n",
        "Vamos con un ejercicio:\n"
      ],
      "metadata": {
        "id": "aUVibssKvReT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcACFNDzxpFL",
        "outputId": "0870e8ed-a4d1-4342-c93d-c36ee99dcb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introducción a la tokenización**\n",
        "La tokenización es el proceso de transformar una cadena o documento en fragmentos más pequeños, a los que llamamos tokens.\n",
        "\n",
        "Este suele ser un paso en el proceso de preparación de un texto para el procesamiento del lenguaje natural.\n",
        "\n",
        "La tokenización hará cosas como romper palabras u oraciones, separar las puntuaciones o incluso se puede tokenizar partes de una cadena, como separar todos los hashtags en un Tweet.\n",
        "\n",
        "Una librería que se usa comúnmente para la tokenización simple es `NLTK`.\n",
        "\n",
        "A continuación puede verse un breve ejemplo del uso del método `word_tokenize`para dividir una cadena en tokens:"
      ],
      "metadata": {
        "id": "tm5bPct4zq-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTIjBGfHyozp",
        "outputId": "61494fbf-0bfa-4d4e-ca5d-f63481a5e931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'there', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver en el resultado que las palabras están separadas y la puntuación también son tokens individuales.\n",
        "\n",
        "**¿Por qué es necesario tokenizar?**\n",
        "\n",
        "Por que puede ayudarnos con algunas tareas simples de procesamiento de texto, como mapear parte del discurso, hacer coincidir palabras comunes o repetidas o eliminar tokens que no queremos.\n",
        "\n",
        "Existen diferentes tipos de tokenizadores en la librería `NLTK`:\n",
        "\n",
        "* `sent_tokenize`: Tokeniza un documento en frases.\n",
        "* `regexp_tokenize`: Tokeniza un string o documento basándose en patrones de expresiones regulares.\n",
        "* `TweetTokenizer`: Es una clase especial para tokenizar únicamente Tweets, permitiéndote separar por hashtags, menciones y otras características.\n",
        "\n",
        "También se aprende como usar regex para analizar los tokens. Algo importante es diferenciar entre cuando usar `re.search()`y `re.match()`:\n",
        "\n"
      ],
      "metadata": {
        "id": "xk2i_VAq1DEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(re.match('abc', 'abcde'))\n",
        "print(re.search('abc','abcde'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qToDGtMk0y3S",
        "outputId": "d483c74c-079f-48c2-ebf3-8f9a8bea6445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "<re.Match object; span=(0, 3), match='abc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El método match da: {re.match(\"cd\",\"abcde\")}')\n",
        "print(re.search('cd', 'abcde'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcslOjAU2hR2",
        "outputId": "dce367fc-9893-428e-bc70-effd16ef0ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El método match da: None\n",
            "<re.Match object; span=(2, 4), match='cd'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, el método `match`no puede buscar patrones en medio de cadenas. Esto se debe a que `match`intentará hacer coincidir una cadena desde el principio hasta que ya no pueda coincidir. La búsqueda pasará por TODA la cadena para buscar opciones de coincidencia.\n",
        "\n",
        "Si necesitas encontrar un patrón que podría no estar al comienzo de la cadena, deberías usar el método `search()`.\n",
        "\n",
        "Si deseas ser específico acerca de la composición de todo la cadena o al menos el patrón incial, entonces deberías usar `match()`.\n",
        "\n",
        "Vamos con un ejercicio:"
      ],
      "metadata": {
        "id": "vJBcHsMZ2uIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\"\"\"La variable scene_one contiene el guión de una película de los Monty Python. Ejercicio realizado en el entorno de DataCamp\"\"\"\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "metadata": {
        "id": "8K3mB9v52tYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())"
      ],
      "metadata": {
        "id": "OkSXOaIL4jaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*\\]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))"
      ],
      "metadata": {
        "id": "VveesA0j5CpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[\\w\\s]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "metadata": {
        "id": "kz4gicu75DZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenización avanzada con regex\n",
        "\n",
        "Un nuevo patrón de expresiones regulares que será útil para la tokenización avanzada es el método OR. En regex, este método se representa usando `|`.\n",
        "\n",
        "Para usarlo, se puede definir un grupo usando `()`. Los grupos pueden ser un patrón o un conjunto de caracteres que desee hacer coincidir.\n",
        "\n",
        "También se puede definir clases de caracteres explícitos mediante `[]`.\n",
        "\n",
        "Hagamos un ejemplo en el que queremos tokenizar usando expresiones regulares y queremos encontrar todos los dígitos y palabras."
      ],
      "metadata": {
        "id": "tl7r1Gve5ZNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "match_digits_and_words = ('(\\d+|\\w+)')\n",
        "re.findall(match_digits_and_words, 'He has 11 cats.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtPWwvtn_BNt",
        "outputId": "4e4953a3-5e04-4ef9-ab6a-b568585cda61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He', 'has', '11', 'cats']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grupos y rangos en regex\n",
        "\n",
        "\n",
        "| pattern | matches | examples |\n",
        "|-----------|-----------|-----------|\n",
        "| [A-Za-z]+ | upper and lowercase English alphabet | 'ABCDEFghijk' |\n",
        "| [0-9] | numbers from 0 to 9 | 9 |\n",
        "| [A-Za-z\\-\\.]+ | upper and lowercase English alphabet, - an . | 'My-website.com' |\n",
        "| (a.z) | a,- and z | 'a-z' |\n",
        "| (\\s+|,) | spaces or a comma | ',' |\n"
      ],
      "metadata": {
        "id": "YUFS3ozH_SW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
        "re.match('[a-z0-9 ]+', my_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmA3-G0R_N0I",
        "outputId": "7b70ff49-4345-407c-eec2-29b6ff0647db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos un ejercicio:"
      ],
      "metadata": {
        "id": "LVaVuZ0fAQFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)"
      ],
      "metadata": {
        "id": "pAs0ELdjAo1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "metadata": {
        "id": "OD6UBja0Aqa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "id": "dR-WSB-QA5BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trazar la longitud de palabra con NLTK\n",
        "\n",
        "En este módulo se enseña como usar gráficos con las herramientas de NLP."
      ],
      "metadata": {
        "id": "atQVJ8PAB7mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(\"This is a pretty cool tool!\")\n",
        "word_lengths = [len(w) for w in words]\n",
        "plt.hist(word_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "iSfKGnrcCL3Z",
        "outputId": "8e50d7c1-85f8-495a-9999-e93c6663636f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
              " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the script into lines: lines\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kft4U5fRCZIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word counts with bag-ofwords**\n",
        "*Bag of words* es un método muy simple y básico para encontrar temas en un texto.\n",
        "\n",
        "Para realizar este método, primero se deben crear tokens mediante la tokenización y luego contar todos los tokens que se tiene.\n",
        "\n",
        "La teoría es que cuanto más frecuente es una palabra o elemento, más central o importante puede ser para el texto.\n",
        "\n",
        "*Bag of words* puede ser una excelñente manera de determinar las palabras significativas en un texto según la cantidad de veces que se usan."
      ],
      "metadata": {
        "id": "bxuNdNG7DKAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat\"\"\"))\n",
        "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5AaVM70FoBx",
        "outputId": "785407b6-cf8a-40ee-cf0e-253c718ae7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'The': 3,\n",
              "         'cat': 3,\n",
              "         'is': 2,\n",
              "         'in': 1,\n",
              "         'the': 3,\n",
              "         'box': 3,\n",
              "         '.': 2,\n",
              "         'likes': 1,\n",
              "         'over': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.most_common(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiv2UABiFwSQ",
        "outputId": "c9d3b69f-8952-4533-e8ec-71458640514c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 3), ('cat', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))"
      ],
      "metadata": {
        "id": "xu21yVriFzcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesado de texto simple\n",
        "El procesamiento de texto ayuda a mejorar los datos de entrada al realizar el aprendizaje autmático u otros métodos estadísticos. Entre las técnicas de procesamiento más comunes se encuentran:\n",
        "\n",
        "* Tokenización para la creación de *bag of words*\n",
        "* Pasar a minúsculas\n",
        "* Lematización/Stemming, donde se acortan las palabras a sus raíces\n",
        "* Eliminación de *stopwords* (eliminar signos de puntuación, artículos...)\n",
        "\n"
      ],
      "metadata": {
        "id": "L8CV0BsXB_-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descargar recursos solo si no están ya descargados\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
        "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
        "\n",
        "result = Counter(no_stops).most_common(2)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86CCXldICxHW",
        "outputId": "76abdc26-78f9-48be-fa69-34c015536986"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('cat', 3), ('box', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio"
      ],
      "metadata": {
        "id": "bFIsxENsFCvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "id": "PEfM--5pDPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción a *gensim*\n",
        "En esta subsección se comienza a usar una nueva herramienta llamada Gensim. Es una biblioteca de preocesamiento de lenguaje de código abierto. Utiliza modelos académicos para realizar tareas complejas como crear documentos o word embbedings, corpus y realización de identificación de temas y comparación de documentos.\n",
        "\n",
        "Gensim permite construir corpus y diccionarios usando clases y funciones simples.\n",
        "\n",
        "Un corpus es un conjunto de textos que se utilizan para ayudar a realizar tareas de procesamiento del lenguaje.\n",
        "\n",
        "### **¿Qué es un **word vector**?\n",
        "\n",
        "Un *word embedding* se entrena a partir de un corpus más grande y es una representación multidimensional de una palabra o documento.\n",
        "\n",
        "Se puede pensar en él como una matriz multidimensional normalmente con pocas características (muchos ceros y algunos unos).\n",
        "\n",
        "Con estos vectores, podemos ver las relaciones entre las palabras o documentos basados en cán cerca o lejos están y también qué comparaciones similares encontramos.  \n",
        "\n",
        "Por ejemplo, podríamos ver que la palabra España es a Madrid lo que Italia es a Roma.\n",
        "\n",
        "El algoritmo de aprendizaje profundo es capaz de destilar este significado en función de cómo se usan esas palabras a lo largo del texto."
      ],
      "metadata": {
        "id": "q4WVAS00FIzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo\n",
        "A continuación, vemos una lista de cadenas que parecen reseñas de películas de ciencia ficción. Vamos a describir el *pipeline* a realizar:\n",
        "\n",
        "1. Procesamiento: por simplicidad, solo tokenización y minúsculas\n",
        "2. Creación de un diccionario. Esto creará una asignación con una identificación para cada token.\n",
        "3. Ahora, podemos representar documentos completos usando solo una lista de sus identificadores de token y con que frecuencia aparecen esos tokens en cada doc.\n",
        "4. Usando ese diccionario, podemos crear un corpus de Gensim. Esto es un modelo un poco más avanzado que las bag of words creadas anteriormente. Gensim usa un modelo simple que transforma cada documento en bolsa de palabras utilizando los identificadores de token y la frecuencia de cada token en el doc. Lo que devuelve es una lista de listas donde cada elemento de la lista representa un documento. Cada documento está compuesto por una serie de tuplas donde el primer elemento representa el tokenid y el segundo la frecuencia."
      ],
      "metadata": {
        "id": "brvRW7YFHPJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "my_documents = ['The movie was about a spaceship and aliens.',\n",
        "                'I really liked the movie!',\n",
        "                'Awesome action scenes, but boring characters.',\n",
        "                'The movie was awful! I hate alien films',\n",
        "                'Space is cool! I liked the movie.',\n",
        "                'More space films, please!']\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "print(dictionary.token2id)\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "print(corpus)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA3DwWsvG0yE",
        "outputId": "e480a8d4-f662-4cd1-aac3-56d8a8e3afee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'.': 0, 'a': 1, 'about': 2, 'aliens': 3, 'and': 4, 'movie': 5, 'spaceship': 6, 'the': 7, 'was': 8, '!': 9, 'i': 10, 'liked': 11, 'really': 12, ',': 13, 'action': 14, 'awesome': 15, 'boring': 16, 'but': 17, 'characters': 18, 'scenes': 19, 'alien': 20, 'awful': 21, 'films': 22, 'hate': 23, 'cool': 24, 'is': 25, 'space': 26, 'more': 27, 'please': 28}\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)], [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vamos con un ejercicio"
      ],
      "metadata": {
        "id": "AaXnEU3qItLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])\n",
        "\n",
        "\n",
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "metadata": {
        "id": "LmpdM4DAIpBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tf-idf con gensim**\n",
        "En esta subsección se explica como usar un modelo TFIDF con Gensim. Tf-idf significa *term frequency - inverse document frequency*. Es un modelo de proecsamiento de uso común que ayuda a determinar las palabras más importantes en cada documento del corpus.\n",
        "\n",
        "La idea detrás de tf-idf es que cada corpus puede tener palabras compartidas que van más allá de simplemente las palabras de tipo *stopword*. Estas *stopwords* son como palabras vacías y por lo tanto, deben eliminarse o reducirse en importancia. Tf idf hace precisamente eso. Tomará textos que compartan un lenguaje común y asegura que las palabras más comunes en todo el corpus no aparecen como palabras clave.\n",
        "\n",
        "Tf-idf ayuda a que las palabras frecuentes tengan un peso alto y las palabras comunes en todo el corpus tengan un peso bajo.\n",
        "\n",
        "La fórmula de tf-idf es la siguiente:\n",
        "\n",
        "\\begin{equation}\n",
        "w_{i,j} = tf_{i,j}  * log(\\frac{N}{d_{if}})\n",
        "\\end{equation}\n",
        "\n",
        "Donde:\n",
        "* w_i,j  es el peso TF-IDF para el token i en el documento j\n",
        "* tf_i,j es el número de ocurrencias del token i en el documento j\n",
        "* df_i es el número de documentos que contienen al token i\n",
        "* N es el número total de documentos.\n",
        "\n",
        "Interpretando la fórmula, el peso será bajo si el término no aparece con frecuencia en el documento, por que la variable tf entonces será baja.\n",
        "\n",
        "Sin embargo, el peso también será bajo si el logaritmo está cerca de cero, lo que significa que la ecuación interna es baja.\n",
        "\n",
        "Aquí podemos ver si el número total de documentos dividido por el número de documentos que tiene el término es cercano a uno, entonces nuestro logaritmo será cercano a 0.\n",
        "\n",
        "Por lo tanto, las palabras que aparecen en muchos o en todos los documentos tendrán un peso tf-idf muy bajo.\n",
        "\n",
        "Por el contrario, si la palabra solo aparece en unos pocos documentos, ese logaritmo devolverá un número mayor.\n",
        "\n",
        "La forma de construirlo con Gensim es el siguiente:"
      ],
      "metadata": {
        "id": "_abU2GRPOm-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "tfidf = TfidfModel(corpus)\n",
        "tfidf[corpus[1]] #acceder al primer documento"
      ],
      "metadata": {
        "id": "hQWv_JKwR8A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio"
      ],
      "metadata": {
        "id": "2AtOsHGlVuD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)\n"
      ],
      "metadata": {
        "id": "9KCKxnsRVvRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconocimiento de entidades nombradas\n",
        "*Named Entity Recognition* o NER es una tarea de NLP usada para identificar entidades nombradas importantes en el texto como personas, lugares y organizaciones e incluso pueden ser fechas, estados, obras de arte y otras categorías según las bibliotecas y notación que se use.\n",
        "\n",
        "NER puede usarse junto con la identificación de temas para determinar importantes elñementos de un texto o responder preguntas básicas del lenguaje.\n",
        "\n"
      ],
      "metadata": {
        "id": "dkRC4O1NdaNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Texto de ejemplo\n",
        "sentence = '''In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Ruth Reichl.'''\n",
        "\n",
        "# Tokenización y etiquetado POS\n",
        "tokenized_sent = nltk.word_tokenize(sentence)\n",
        "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
        "\n",
        "# Imprimir las primeras tres palabras etiquetadas\n",
        "print(tagged_sent[:3])\n",
        "\n",
        "# Etiquetado de entidades nombradas (NER)\n",
        "ner_tree = nltk.ne_chunk(tagged_sent)\n",
        "print(ner_tree)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmQpAF3GeUcH",
        "outputId": "e9207580-e783-4d93-bda7-8191318c67ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n",
            "(S\n",
            "  In/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  like/VBP\n",
            "  to/TO\n",
            "  ride/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Metro/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (ORGANIZATION MOMA/NNP)\n",
            "  and/CC\n",
            "  some/DT\n",
            "  restaurants/NNS\n",
            "  rated/VBN\n",
            "  well/RB\n",
            "  by/IN\n",
            "  (PERSON Ruth/NNP Reichl/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = nltk.sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)\n"
      ],
      "metadata": {
        "id": "5zQ--YDnenD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}